# ArianeAI â€“ Journal d'installation (FR / EN)

## ðŸ‡«ðŸ‡· Version FranÃ§aise

### ðŸ“… JournÃ©e d'installation â€“ RÃ©sumÃ© complet

Cette journÃ©e marque la crÃ©ation de **ArianeAI**, ton IA souveraine fonctionnant entiÃ¨rement en local sur ton Dell Optiplex 3020. Voici la documentation complÃ¨te des Ã©tapes rÃ©alisÃ©es.

---

## ðŸ–¥ï¸ 1. Installation de Linux Mint 21.3 Â« Virginia Â»

- TÃ©lÃ©chargement de lâ€™ISO Linux Mint 21.3 (Ã©dition Cinnamon)
- CrÃ©ation dâ€™une clÃ© USB bootable via Rufus
- Boot sur la clÃ© & installation en **dual-boot avec Windows 10**
- Linux dÃ©tecte automatiquement ta carte Wi-Fi Intel
- Mises Ã  jour systÃ¨mes effectuÃ©es

---

## ðŸ¤– 2. Installation dâ€™Ollama

Commande utilisÃ©e :

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

Test du modÃ¨le local :

```bash
ollama run llama3
```

Le modÃ¨le **llama3:latest (Q4)** fonctionne correctement.

---

## ðŸŒ 3. Configuration dâ€™Ollama pour l'accÃ¨s rÃ©seau

Modification du service systemd :

Fichier : `/etc/systemd/system/ollama.service`

Ajout :

```ini
Environment="OLLAMA_HOST=0.0.0.0"
ExecStart=/usr/local/bin/ollama serve
```

Puis :

```bash
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

Test local :

```bash
curl http://localhost:11434/api/tags
```

Test depuis une autre machine :

```bash
curl http://192.168.2.110:11434/api/tags
```

---

## ðŸ”¥ 4. Premier appel Nuxt â†’ Dell â†’ Ollama

CrÃ©ation dâ€™un endpoint Nuxt :

`server/api/ai/generate.post.ts`

```ts
export default defineEventHandler(async (event) => {
  const body = await readBody(event)

  const response = await $fetch('http://192.168.2.110:11434/api/generate', {
    method: 'POST',
    body: {
      model: 'llama3',
      prompt: body.prompt,
      stream: false
    }
  })

  return response
})
```

Test cÃ´tÃ© front :

```ts
const { data } = await useFetch('/api/ai/generate', {
  method: 'POST',
  body: { prompt: 'Test depuis Nuxt' }
})
console.log(data.value)
```

RÃ©sultat : **SuccÃ¨s !**

---

## ðŸ”’ 5. SÃ©curisation rÃ©seau (UFW)

```bash
sudo apt install ufw -y
sudo ufw allow from 192.168.2.0/24 to any port 11434
sudo ufw enable
```

---

## ðŸŽ‰ Conclusion

Tu disposes maintenant de :

- Une machine Linux prÃªte pour servir d'IA locale  
- Ollama configurÃ© proprement  
- Un endpoint Nuxt opÃ©rationnel  
- Une architecture de base pour **ArianeAI**, ton IA souveraine  

---

# ðŸ‡¬ðŸ‡§ English Version

### ðŸ“… Installation Day â€“ Full Documentation

This day marks the creation of **ArianeAI**, your fully local sovereign AI running on your Dell Optiplex 3020. Below is the full documentation.

---

## ðŸ–¥ï¸ 1. Installing Linux Mint 21.3 â€œVirginiaâ€

- Downloaded Linux Mint 21.3 ISO (Cinnamon edition)
- Created bootable USB with Rufus
- Booted and installed in **dual-boot with Windows 10**
- Intel Wi-Fi card automatically detected
- Completed system updates

---

## ðŸ¤– 2. Installing Ollama

Command used:

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

Test model:

```bash
ollama run llama3
```

Result: **llama3:latest (Q4)** works correctly.

---

## ðŸŒ 3. Configuring Ollama for LAN access

Edited systemd service:

`/etc/systemd/system/ollama.service`

Added:

```ini
Environment="OLLAMA_HOST=0.0.0.0"
ExecStart=/usr/local/bin/ollama serve
```

Reloaded & restarted:

```bash
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

Local test:

```bash
curl http://localhost:11434/api/tags
```

Remote test:

```bash
curl http://192.168.2.110:11434/api/tags
```

---

## ðŸ”¥ 4. First Nuxt â†’ Dell â†’ Ollama request

Nuxt API route:

```ts
export default defineEventHandler(async (event) => {
  const body = await readBody(event)

  const response = await $fetch('http://192.168.2.110:11434/api/generate', {
    method: 'POST',
    body: {
      model: 'llama3',
      prompt: body.prompt,
      stream: false
    }
  })

  return response
})
```

Client-side test:

```ts
const { data } = await useFetch('/api/ai/generate', {
  method: 'POST',
  body: { prompt: 'Test from Nuxt' }
})
console.log(data.value)
```

Result: **Success!**

---

## ðŸ”’ 5. Firewall configuration

```bash
sudo apt install ufw -y
sudo ufw allow from 192.168.2.0/24 to any port 11434
sudo ufw enable
```

---

## ðŸŽ‰ Conclusion

You now have:

- A fully functioning Linux server  
- Ollama running and reachable over LAN  
- Nuxt properly connected to your AI backend  
- A stable foundation for **ArianeAI**, your sovereign AI  

---

## ðŸ“„ End of Documentation
